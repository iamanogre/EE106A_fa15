 <!DOCTYPE html>
 <html>
 
 <head>
 	<title>Zumy Light Exploration</title>
 
 	<!-- CSS --> 
 	<link rel="stylesheet" href="css/skeleton.css">
 	<link rel="stylesheet" href="css/custom.css">

  <!-- JS -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>

    <script>
    $(document).ready(function(){
      $("#garv").hover(function(){
          $("#description").empty();
          $("#description").append('<p class="describe">Gary Choi is a Senior in Electrical Engineering and Computer Science at UC Berkeley. He loves low-level architecture and hopes to work at Microsoft one day. Recent survivor of EECS151 and May 2016 graduate.</p>')
      });
      $("#gary").hover(function(){
          $("#description").empty();
          $("#description").append('<p class="describe">Gary Hoang is a Junior in EECS at UC Berkeley. Gary also wants to go to space one day. He also wrote this website with Gary Choi\'s help. Recent survivor of EECS151.</p>')
      });
      $("#xavier").hover(function(){
          $("#description").empty();
          $("#description").append('<p class="describe">Xavier Lavenir is a Senior in Mechanical Engineering at Imperial College London. He will be graduating in May 2016 with a Masters. Xavier also believes MechEng to be the one true engineering.</p>')
      });
    });
  </script>
 </head>
 <body>
 
 <!-- .container is main centered wrapper -->
 <!--
 <div class="left" style="background-color:red"></div>
-->

 <div class="container" style="width:100%;word-wrap:break-word;">
 
    <div class="row" style="position:fixed;background-color:#FFFFFF;margin-top:-10px;margin-bottom:-10px;z-index:99">
      <div class="twelve columns">
        <a class="button" href="#intro"> Introduction</a>
        <a class="button" href="#design">Design</a>
        <a class="button" href="#implementation">Implementation</a>
        <a class="button" href="#result">Results</a>
        <a class="button" href="#conclusion">Conclusion</a>
        <a class="button" href="#team">Team</a>
        <a class="button" href="#add_materials">Additional Materials</a>
      </div>
    </div>

    <!-- Abstract and preview so people know what's happening-->
    <div class="row" style="padding-top:2em;" id="home">
      <div class="twelve columns">
        <h1>Zumy Light Exploration <span style="float:right">EE106A Fall 2015 Final Project</span></h1>
        <p style="padding-top:2em;"></p>
        <p style="text-align:center;"><img src="img/Front_pic_v1.jpg" alt="Zumy" style="width:100%; height:auto;"></p>
       	<h3 style="text-align:center;margin-top:-2.1em;text-shadow: -1px -1px 0 #000, 1px -1px 0 #000, -1px 1px 0 #000, 1px 1px 0 #000; color:white;opacity:0.9;"><em>"simulating the exploration of an unknown planet"</em></h3>
      </div>
    </div>

    <p class="excess" id="intro"></p> 
    <div class="row">
      <div class="twelve columns">
        <h2 class="header">1. Introduction</h2>
        <p>The goal of this project is to successfully launch an autonomous network of three Zumies (small mobile robots, as seen
        above) to explore a surrounding area in search of light. The purpose of searching for light is to recharge the Zumies' solar 
        cells and effectively keep each other running whilst exploring an unknown enviroment. Interesting features of
        this project are the ROS (Robot Operating System) network that connects, coordinates, and runs the Zumies as well as the light
        sensing and position tracking components.</p>

        <p>Our mission is to one day have the Zumies on another planet, aiding in autonomous planet exploration with the 
        self-sustainable tactic of locating light and communicating to each other the best source of light. </p>

        <p>Here is the final video.</p> 
        <div class="vid">
          <iframe width="800" height="500"
          src="https://www.youtube.com/embed/0k9ZuHElfLA" >
          </iframe>
          </div>

      </div>
    </div>
 
    <p class="excess" id="design"></p> 
   	<div class="row">
     	<div class="twelve columns">
        <h2 class="header">2. Design</h2>

        <h5 class="under">Simple Terminology</h5>
          <p><strong>Zumy</strong> - a small mobile robot. Relevant features are the Linux OS on the robot to interface with ROS, USB connectors 
          for a camera, a wifi adapater to communicate with a network, pins for analog input, and an internal gyroscope and accelerometer originally meant to communicate with the Kalman filter. 
          <p><strong>Photoresistor</strong> - light sensitive resistor. Has variable resistance that changes with exposure to light</p>
          <p><strong>Voltage divider circuit</strong> - simple circuit taking advantage of a photoresistor's dynamic resistance to output a variable voltage</p>

          <p style="text-align:center;"><img src="img/termin.png" alt="Flow Chart" style="width:90%; 
          height:auto;"></p>

        <h5 class="under">Initial Design</h5>
        <p>in an effort to simulate realistic exploration, our initial design was to mount cameras and four photocells onto each Zumy instead of using a global camera. AR tags placed in the field of exploration would be objects of known positions - landmark AR tags. 
        From these AR tags, a Zumy can determine it's position within the field. Further, when a Zumy does not see an AR tag, and 
        thus cannot determine its position accurately from its camera readings, it would instead use the Kalman Filter, integrating
        readings from its accelerometer to accurately calculate its position.</p>
        <p>In terms of exploring, the three Zumies would explore an equal region in the field and when one of them found a sufficient
        source of light to charge its solar cells, it would signal to the others to stop searching. The other Zumies would stop
        searching, and having received the position of its fellow Zumy, they would navigate over to the source of light. </p>

        <h5 class="under">Final Design</h5>
        <p>The project was an overall successful and the majority of the initial objectives were met – the most notable difference 
        with the initial design is that the Kalman Filter was not implemented. This is because the readings from the accelerometer 
        of the IMU (Inertial Measuring Unit) were found to be unreliable. It was decided that a global camera, overlooking the entire
        exploration area would be used to simulate the Kalman Filter. Similarly, it would only provide information to the Zumy when 
        the mounted camera could not see an AR tag.</p>
        <p>Furthermore, although it was initially planned to use multiple Zumies searching an area for light, only one Zumy was used 
        for photo-detection as there were limited Zumies available in the department. All of the software was designed to allow
        multiple Zumies to explore and thus it is easy to expand the project to multiple Zumies. A second Zumy, without light sensors 
        or a mounted camera, was added and told to explore to show that once a Zumy discovers a strong enough light source, all 
        exploration Zumies would assemble at this point.</p>
        <p>The table below shows the main differences of the final design relative to the initial objectives:</p>
        <table class="u-full-width">
          <thead>
            <td></td>
            <td><strong>Achieved</strong></td>
            <td><strong>Objective</strong></td>
          </thead>
          <tr>
            <td><strong>Positioning System</strong></td>
            <td>4 Landmark AR tags<br>Mounted Camera<br>Global Camera</td>
            <td>Landmark AR tags<br>Mounted Cameras<br>Kalman Filter</td>
          </tr>
          <tr>
            <td><strong>Light Sensing</strong></td>
            <td>1 Zumy with 4 light sensors</td>
            <td>Each Zumy has light sensors</td>
          </tr>
          <tr>
            <td><strong>Exploration</strong></td>
            <td>Split region between 2 Zumies</td>
            <td>Split region between Zumies</td>
          </tr>
        </table>

        <p>The photograph below shows the final setup which was used for the project. It should be noted that the global camera cannot
        be seen but is overlooking the exploration region. Of further note is that the region is split equally in two parts between
        the Zumies. The top part is explored by the un-sensorised whilst the lower part is explored by the Zumy with sensors. A patch
        of bright light was added so that the Zumies stop exploring once one of them has been found it.</p>

        <p style="text-align:center;"><img src="img/setup.png" alt="Setup" style="width:90%; height:auto;"></p>

        <p><strong>Zumy Setup</strong><br>The Zumy was modfied in order to attach light sensors, an AR tag, and a camera. These modifications are shown in the figures below.</p>

        <p style="text-align:center;"><img src="img/ZumyIllustration.jpg" alt="Zumy" style="width:90%; height:auto;"></p>

        <p style="text-align:center;"><img src="img/setup2.png" alt="Setup2" style="width:90%; height:auto;"></p>

        <p>The four photoresistors used to detect light were connected to the Analog inputs pins on the Zumy’s microprocessor via a
        breadboard as shown in the figure below.</p>

        <p style="text-align:center;"><img src="img/setup3.png" alt="Setupe3" style="width:100%; height:auto;"></p>

               <h5 class="under">Design Criteria</h5>
        <p>The final design choices meet several design criteria relatively well and others not so well. For example, our system is not entirely
        robust. The code is heavily dependant on the existing ROS software but if any node, other than the Zumy node, crashes, the
        entire system will hang. There is one point of robustness and this is if the Zumy does not see an AR tag, the global camera will still capture its position.</p>
        
        <p>
        
        Despite this setback, our design is clean and efficient. We had the explorer node (covered in more detail below) assign sections of exploration so the Zumies do not overlap in their exploration regions and search the same areas 
        twice. The intent was to find light to charge the Zumies, so having them conserve energy while looking for places to charge is 
        an issue our design addresses. Further, our design is easily expandable because the code is
        modulized. Thus, changing the number of exploring Zumies or changing the exploration
        algorithm is easy to locate and accomplish. We can further adapt the controller node (also discussed in more detail below) to send different information to our explorer node for better
        feedback.
        
        </p><p>
        
        Some of the design criteria that we fail to meet in a real-world robotics setting is due to the fact that our system is built largely from existing implementation, namely ROS. For example, problems such as 
        recovering from crashes in ROS is not something we have knowledge of.  </p><p> Ultimately, we believe we did well to meet design criteria that we did have control of.</p>
        
         </p>
      </div>
    </div>

    <p class="excess" id="implementation"></p> 
   	<div class="row">
     	<div class="twelve columns">
        <h2 class="header">3. Implementation</h2>

          <h5 class="under">Positioning System</h5>

          <p>The final positioning system consisted of using the camera mounted on the Zumy as well as a global camera overlooking 
          the region to be explored. The global camera is only used to update the position of the Zumy when no landmark AR tags 
          are in the field of view of the mounted camera – as previously mentioned, this is done to simulate the Kalman filter.</p>
          <p>The state of the Zumy can be described using 3 different parameters: the x coordinate, the y coordinate, and the 
          orientation denoted by theta. All parameters are measured relative to the origin as shown in the figure below – 
          therefore a positive theta is measured from the positive y-axis for a positive rotation around the z-axis.</p>

          <p>The following sections discuss how each of these methods are used to estimate the positions of the Zumy.</p>

          <h5 class="under">Mounted Camera</h5>
          <p>As seen in the figure below, the mounted camera is used to calculate the transform from the Zumy to the Landmark AR 
          tag. As the AR tags are placed at known positions and orientations around the region to be explored, it is possible to
          estimate the Zumy’s position. Each time the Zumy sees one of these AR tags, it calculates (using forward kinematics) the
          transform from the origin to itself and thus, both the position and orientation of the Zumy, relative to the origin, can
          be inferred. </p>

          <p style="text-align:center;"><img src="img/rviz.png" alt="Flow Chart" style="width:90%; height:auto;"></p>

          <p>The figure below shows the case where the Zumy is facing AR tag 3. The position can then be easily calculated using 
          the formula below because the transform from the origin to the AR tag is known and the Zumy’s camera can measure the
          transform the Zumy to the AR tag.</p>

          <p style="text-align:center;"><img src="img/equations1.png" alt="Flow Chart" style="width:100%; height:auto;"></p>

          <p>To calculate the orientation of the Zumy, a simple addition or subtraction from the angle measured by the mounted 
          camera is required (the amount to add or subtract depends on which AR is in view) – this is so that theta is measured
          anti-clockwise from the positive y-axis of the origin.</p>

          <p style="text-align:center;"><img src="img/position_track1.png" alt="Flow Chart" style="width:90%; height:auto;"></p>

          <h5 class="under">Global Camera</h5>
          <p>The position and orientation of the Zumy relative to the origin can easily be calculated using the AR tag located on
          the Zumy and the AR tag located at the origin. The transform from the origin to the Zumy can be calculated using the
          following below.</p>
          
          <p style="text-align:center;"><img src="img/equations2.png" alt="Flow Chart" style="width:100%; height:auto;"></p>
          <p>The orientation of the Zumy can be extracted from the transform from the origin to the Zumy. </p>

          <h5 class="under">Motion and Exploration</h5>

          <p>Another important section in this project was the exploration phase. The main goals consisted of telling the Zumy to 
          move to certain locations within the exploration region and measuring the local light intensity as it navigates. </p>
          <p>The first step was to tell the Zumy to move from one point within our region to another. The figure below shows the
          case where the Zumy starts off at point A with orientation theta and needs to move towards point B. The flow chart below
          illustrates the algorithm which controls how the Zumy moves from point A to point B. θd is the angle which the Zumy must
          face within a certain bound to move forward, as illustrated in the figure below - it can easily be calculated using 
          simple trigonometry. </p>

          <p style="text-align:center;"><img src="img/flowChart.png" alt="Flow Chart" style="width:100%; height:auto;"></p>

          <p>Essentially, the Zumy is told to rotate until it is facing the desired location within a certain bound. The Zumy 
          will then proceed to move forward and continuously makes small adjustments in its orientation in order to advance in
          the correct direction, until it reaches its target location. The software was written such that the Zumy always rotates in
          the direction closest to the desired orientation.</p>
          <p style="text-align:center;"><img src="img/position_track2.png" alt="Flow Chart" style="width:90%; height:auto;"></p>


          <p>The video below shows a test in which the Zumy is told to move from one location to another.</p>
          <div class="vid">
          <iframe width="800" height="500"
          src="https://www.youtube.com/embed/ux2dBUnrHBU" >
          </iframe>
          </div>
          <p>The next stage is to split the exploration region between the two Zumies and have one move towards the other when
          a moving average of the light readings exceeds a certain threshold value. After calibrating the sensors, the readings 
          from each photo sensor ranged from around 0.2 in the darkness up to 1.0 in high light intensity. A threshold value of
          0.7 seemed to produce reasonable results. Since the region to be explored was rather small, each Zumy was assigned to 
          half the total area and each would explore its area randomly as shown in the video below.</p>

          <div class="vid">
          <iframe width="800" height="500"
          src="https://www.youtube.com/embed/DWWPY7QQbqk" >
          </iframe>
          </div>
        
          
        <h5 class="under">System Architecture</h5>
        <p>Our software is primarily constructed from ROS packages and consists of topics, nodes, and services. All of the programs we wrote were written in Python.</p>
        <p><strong>Topics</strong><br>The following are the ROS topics that publish our system information.</p>
        <ul>
        <li><strong>zumyxx/photo</strong><br>Message Published: Float32MultiArray [found in std_msgs]
        <br> - The 'xx' in the topic refers to the respective Zumy. For example, zumy3b/photo is photo data published by Zumy3b. The required files to upload onto the Zumy (zumy.py and zumy_ros_bridge.py) can be found in section 7, Additional Materials.
        <br> - Float32MultiArray is a complex data structure, consisting of two data types. The first is layout (called through Float32MultiArray.layout), specifing the layout of the data of the array. The second is data (called through Float32MultiArray.data), which contains the actual float array. The format of this float array, for our purposes, is a four-element list of floats. Each float represents the analog value read from each photo resistor on the zumy. </li>
        <li><strong>tf</strong><br>Message Published: TFMessage [found in tf2_msgs]
        <br> - There are a variety of nodes that publish onto this topic.
        <br> - The first nodes are usb_cam_zumyxx. Once again, "xx" denotes the correspoding Zumy. Thus a TFMessage from usb_cam_zumy3b is a TFMessage from Zumy3b's camera readings.
        <br> - The other node that publishes to the tf topic is the global camera. The Transform encapsulated in it's TFMessage is the Transform between the origin tag to the Zumy's tag in the field.
        <br> - TFMessage is also another complex message type. The fields to pay close attention to are its TFMessage.transforms[0].child_frame_id, which contains the publishing node (usb_cam or usb_cam_zumyxx) and TFMessage.transforms[0].transform, which contains the Transform.
        </li>
        <li><strong>zumyxx_cmd_vel</strong>
        <br>Message Published: Twist [found in geometry_msgs]
        <br> - Publishing a Twist to zumyxx_cmd_vel makes the Zumy move. 
        <br> - Of key note are that the Zumy cannot move sideways, and thus, only values written to Twist.linear.x (forward speed) and Twist.angular.z (rotation speed) are understood by the Zumy.

        </ul>
        <p><strong>Nodes</strong><br>Although there are many nodes present in our system, such as the camera nodes and service
        nodes, one of these nodes requires greater explanation and is outlined below.</p>
        <ul>

        <li><strong>Controller</strong>
        <br> - Each Zumy in the system has its own controller node. This controller is responsible for interfacing with all other nodes and services. Services are explained below. 
        <br> - Of key note is that this controller node grabs all of the information, specifically the photo data readings and the Zumy's current position, sends this to an Explorer Service, receives back the position it should make the Zumy go to, and finally, calculates and publishes the correct twist to the Zumy.</li>
        </ul>
        <p><strong>Services</strong>
        <br>The following are the services that enable our system information exchange and essentially run the system. 
        Services are unique because unlike topics, information is not continuously published. The exchange occurs when a service request is prompted and a service response follows. An analogy is that services are similar to a handshake between nodes.
        <br><br>The services below indicate the side that initiates first and which effectively sends the request.</p>
        <ul>
        <li><strong>tf_service</strong> - Controller Initiated
        <br>Service Request: string zumy
        <br>Service Response: bool updated, float32 theta, float32[] position
        <br> - The purpose of this service is to tell the Controller its Zumy's current position and rotation in the area. The service request states for which Zumy this request is for and is a string (string zumy). The response is an updated flag, the rotation the Zumy currently makes with the +Y-axis, and the Zumy's position in a two-element float array.
        <br> - The tf_service is able to calculate the Zumy's position by listening on the tf topic. Because it is capable of looking from where the TFMessage is published (usb_cam or usb_cam_zumyxx), the tf_service follows the procedure outlined in the "Mounted Camera" section above, turning the transform into the correct theta and position, stores it in it's own internal dictionary, where the Zumy is the key, and when prompted, returns this position and theta to the correct Zumy response. 
        <br> - The update flag is used to indicate to the Controller that the information the Controller is currently receiving from the tf_service is new information. In the case when the information is stale (because the Zumy's camera no longer sees a landmark AR tag), the Controller will ask the global_serivce for the Zumy's position.
        </li>
        <li><strong>global_service</strong> - Controller Initiated
        <br>Service Request: string zumy
        <br>Service Response: bool updated, float32 theta, float32[] position
        <br> - The global_service works similarly to the tf_service. It listens for tf's on the tf topic from the global camera (usb_cam), and through the method outlined in the "Global Camera" section above, it computes the Zumy's position and theta. 
        <br> - Similar to the tf_service, it stores these values in a dictionary, where the Zumy string name is the key, and returns to the Controller the Zumy's position and rotation through the same service request and response.
        </li>
        <li><strong>explore_service</strong> - Controller Initiated
        <br>Service Request: string zumy, float32[] position, float32[] photo_data, bool finished
        <br>Service Response: float32[] position, bool hard_stop
        <br> - The explore_service is the brain of the system. All Controllers message the explore_service with their Zumy's position and photo data, and the explore_service calculates the desired positions the Zumy's should explore. <br> - 
        In its response, it tells the Zumy whether it should keep exploring by sending it to a new position or by keeping it still at its current position. When a Zumy's photo data reaches over the necessary light threshold, the explore_service tells that Zumy to stop and messages the other Zumy the position of that Zumy. 
        <br> - Furthermore, this service can issue a hard_stop to Zumies, which tells a Zumy to stop moving. Because it knows the positions of all of the Zumies, it can tell them to hard_stop when they would collide and lets one of them go. <br>It is truly the brain of the operation.</li>

        </ul>

        <p><strong>Launch Files</strong>
        <br>The following are the launch files that are launched to begin running our system.</p>
        <ul>
        <li><strong>run_all.launch</strong>
        <br>This launch file is used to launch the global camera. It sits in the tfservice package.
        </li>
        <li><strong>remote_zumy.launch</strong>
        <br>This launch file is used to launch the zumy. It sits in the odroid_machine package.
        </li>

        <p style="padding-top:1em;"><strong>System Architecture Visual</strong><br>
        The following is a visual of the topics, nodes, and services in our system architecture. </p>
        <p><strong>Legend</strong><br><span style="color:black;">Black</span> arrows denote topics.<br>Nodes are circled.<br><span style="color:red;">Red</span> arrows denote service exchanges.</p>
        <p style="text-align:center;"><img src="img/Service_Explorer_Diagram.jpg" alt="Flow Chart" style="width:100%; 
        height:auto;"></p>
        
      </div>
   	</div>
 
    <p class="excess" id="result"></p> 
   	<div class="row">
     	<div class="twelve columns">
        <h2 class="header">4. Results</h2>
        <p>The project was a success. We managed to get 1 Zumy to explore an area marked 
out with Landmark AR tags and an overlooking global camera as well as measure levels of light intensity as it explores. When the Zumy records a level of light above a certain threshold, it stops and tells the other Zumies to join it. </p><p>We only had 1 sensorized Zumy and 1 
unsensorized Zumy simply because it meant that we had to modifiy the hardware on another Zumy extensively and would further render this Zumy unusable to other groups. </p><p>The code and system architecture is also established in a maner that allows easy modification and expansion.</p>
        <p>The video below shows the final result of the project. It is the same video at the top of the web page.</p>
        <div class="vid">
          <iframe width="800" height="500"
          src="https://www.youtube.com/embed/0k9ZuHElfLA" >
          </iframe>
        </div>


      </div>
   	</div>
 
    <p class="excess" id="conclusion"></p> 
   	<div class="row">
     	<div class="twelve columns">
        <h2 class="header">5. Conclusion</h2>
        
        <h5 class="under">Summary</h5>
        <p>The goal of this project was to have Zumies explore an area to find light and signal to other Zumies to go to it. Our system accomplishes this goal. There were a few problems, such as the Kalman Filter, that lead to deviations from our original plan, but 
        ultimately, our system is modulized in a way that makes it relatively simple to switch out the alterations. There is 
        definitely room for improvement and there are others ways to use it beyond its original functionality. One of the most prominent is using the Zumies to
        carry plants to well-lit areas throughout the day and to possibly carry these plants on other planets.</p>
        
        <h5 class="under">Initial Designs, Problems, and Fixes </h5>

        <p>At the start of our project, we had an overall goal, a general approach of how to reach it, and the tools to implement it. 
        However, along the way, we ran into multiple problems. To solve them, we had to make changes to our original design to get around to our final goal. The changes are outlined in greater detail in the table below.</p>
        <table class="u-full-width">
          <thead>
            <td><strong>Original Design and Problem</strong> </td>
            <td><strong>Temporary Fix</strong></td>
            <td><strong>More Permanent/Desirable Solution</strong></td>
          </thead>
          <tr>
            <td>Use the IMU service to estimate position. However, the Kalman Filter, based off accelerometer readings, was not very accurate, making position tracking difficult.</td>
            <td>Used a global camera. We required an AR tag on each Zumy as well as an origin tag.</td>
            <td>- Improved filter, accelerometer, and gyroscope<br>- Use wheel encoders to estimate 
            distance</td>
          </tr>
          <tr>
            <td>Originally planned to not use an origin AR tag with the global camera and instead use a Landmark AR tag for reference. Camera difficulties with angles and RVIZ displays lead to poor, fluctuating readings.</td>
            <td>AR tag at the origin as the primary reference for the global camera and the second Zumy required a mounted AR tag.</td>
            <td>Solutions similar to the above.</td>
          </tr>
          <tr>
           <td> Use 3 Zumies. Lack of available dedicated Zumies made it difficult to secure 3 to setup sensors on.</td>
            <td>Used 2 Zumies, one with light sensors be the one to "find" the light and one without.</td>
            <td>Current code can easily be expanded to account for more Zumies and setting up more light sensors on another Zumy requires medium overhead.</td>
          </tr>
        </table>

        <h3>Future Improvements</h3>
        <ul>
          <li>Combine all services into one package that takes all the information around the system, parses through all of it, and
          simply issues each Zumy a twist.</li>
          <li>Implement more permanent solutions, i.e. remove the global camera and use the Kalman Filter.</li>
          <li>Eliminate the use of AR tags by simply using image detection algorithms and by only using the cameras on the Zumies.</li>
          <li>Improve the exploration algorithm. The current exploration is random and this is not desirable for bigger areas of 
          exploration.</li>
          <li>Add obstacles and implement obstacle avoidance. However, the obstacles must require AR tags. Implementing an image detection algorithm would solve all of these problems.</li>
          <li>Go up the light gradient. Instead of stopping once the light sensor detects a certain threshold, lower the 
          threshold and have the Zumies detect lower thresholds and move up a gradient to find brighter areas. This allows us to better
          find distant light sources. </li>
          <li>Land on Mars. </li>
        </ul>
      </div>
   	</div>
 
    <p class="excess" id="team"></p> 
   	<div class="row">
     	<div class="twelve columns">
        <h2 class="header">6. Team</h2>
      </div>
    </div>
    <div class="row" style="padding-bottom:2em;">
      <div class="four columns" style="text-align:center;float:left;" id="garv"><img src="img/GaryChoi.jpg" alt="Zumy" style="width:100%; height:auto;"></div>
      <div class="four columns" style="text-align:center;" id="gary"><img src="img/GaryHoang.jpg" alt="Zumy" style="width:100%; height:auto;"></div>
      <div class="four columns" style="text-align:center;" id="xavier"><img src="img/XavierLavenir.jpg" alt="Zumy" style="width:100%; height:auto;"></div>
      </div>
      <div class="row">
      <div class="twelve columns" style="text-align:center;" id="description">
        <p class="describe">Hover over a picture to read a bio!</p>
      </div>
 	  </div>
     
    <p class="excess" id="add_materials"></p> 
   	<div class="row">
     	<div class="twelve columns">
        <h2 class="header">7. Additional Materials</h2>
        <h5 class="under">Code</h5>
        <p>Freely available for anyone to use. Simply download project_code.zip and zumy_code.zip.<br>
        <a href="https://github.com/iamanogre/EE106A_fa15" target="_blank">Link</a> to github.</p>
        <p><em>project_code.zip</em> contains the project files to launch the global camera, Zumies, and services.<br>
        <em>zumy_code.zip</em> contains the Zumy code to put on to the Zumy. Contained are zumy.py and zumy_ros_bridge.py</p> 
        <h5 class="under">How to Run Guide</h5>
        <ol>
        <li>Download the code from the section above.</li>
        <li>Unzip the zip files. </li>
        <li>Go to <em>ros_workspaces/project/src/tfservice/launch </em>and change the directory in the <em>lifecam.yml</em> in your machine.</li>
        <li>cd into the <em>ros_workspaces/project</em> and remove the <em>build</em> and <em>devel</em> folders. Run 'catkin_make'. Remember to source the setup.bash file in this directory.</li>
        <li>I will denote 'zumyaa' as the first Zumy and 'zumybb' as the second Zumy. Also 'x' is the width of your area divided by 2, and 'y' is the height of your area divided by 2.</li>
        <li>Launch the global camera by running 'roslaunch tfservice run_all.launch.' Launch the 2 Zumies by running 'roslaunch odroid_machine remote_zumy1.launch mname:=zumyaa' and 'roslaunch odroid_machine remote_zumy2.launch mname:=zumybb'. The remote_zumy1.launch is for the sensorized Zumy. The second is for the unsensorized Zumy.</li>
        <li>Begin the tf_service by running in a new terminal, 'rosrun tfservice tf_srv.py zumyaa zumybb x y'.</li>
        <li>Run the global_service by running in a new terminal, 'rosrun tfservice global_srv.py zumyaa AR_tag_on_zumyaa zumybb AR_tag_on_zumybb AR_tag_at_origin'.</li>
        <li>Run the explore_service by running in a new terminal, 'rosrun tfservice explore_srv.py front_threshold_value back_threshold_value x y'.</li>
        <li>Start the controller on zumyaa by running 'rosrun controller.py zumyaa explore1'.</li>
        <li>Start the controller on zumybb by running 'rosrun controller.py zumybb explore2'.</li>
        <li>Now watch the Zumies search for light!</li>
        </ol>
        <h5 class="under">Photocells</h5>
        <p><a href="https://www.adafruit.com/product/161" target="_blank">Link</a> to Adafruit.</p>
      </div>
   	</div>
 
 </div>

 </body>
 </html>
